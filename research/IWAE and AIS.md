## Metadata
* **Topic 1:** Importance Weighted Autoencoders (IWAE)
* **Topic 2:** Annealed Importance Sampling (AIS)

## Notes
### Importance Weighted Autoencoders
* An **Importance Weighted Autoencoder** (IWAE) is a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting.
    * The goal of a [VAE](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) is to maximize `log p(x) = log ğ”¼ [p(x|z) * p(z) / q(z|x)]` where `z ~ q(z|x)`.
        * This expression for the log-likelihood can be derived as follows:
            ```
            log p(x) = log âˆ« p(x,z) dz
                     = log âˆ« p(x,z) * q(z|x) / q(z|x) dz
                     = log ğ”¼ [p(x,z) / q(z|x)]
                     = log ğ”¼ [p(x|z) * p(z) / q(z|x)]
            ```
        * Computing `log p(x)` directly is intractable because of the integration.
    * The ELBO serves as a lower bound on `log p(x)` by [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).
        * Traditionally, `ğ“›(x) = ğ”¼ [log p(x,z) / q(z|x))]` where `z ~ q(z|x)`.
        * Maximizing the ELBO is equivalent to maximizing `log p(x)`.
    * An IWAE reduces the gap between `log p(x)` and the ELBO by taking the latent expectation over *multiple samples* instead of just one sample.
        * Define `ğ“›â‚–(x) = ğ”¼â‚– [log (1/k * Î£áµ¢ p(x,záµ¢) / q(záµ¢|x))]` where `zâ‚,...,zâ‚– ~ q(z|x)`.
        * The traditional ELBO is recovered when `k = 1`.
    * Crucially, the estimator `ğ“›â‚–(x)` has some desirable statistical properties:
        * `ğ“›â‚–(x)` is consistent and eventually converges to `log p(x)`.
            * As a consequence of the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers),
                ```
                ğ“›â‚–(x) = ğ”¼â‚– [log (1/k * Î£áµ¢ p(x,záµ¢) / q(záµ¢|x))]
                      = ğ”¼â‚– [log ğ”¼ [p(x,z) / q(z|x)]]           // k â†’ âˆ
                      = ğ”¼â‚– [log p(x)]                          // Definition
                      = log p(x)                               // Constant
                ```
            * Additionally, `ğ“›â‚(x) <= ğ“›â‚‚(x) <= ... <= ğ“›â‚–(x) <= ... <= log p(x)`.
            * This implies that `ğ“›â‚–(x)` approaches `log p(x)` as `k â†’ âˆ`
        * The bias of `ğ“›â‚–(x)` vanishes at a rate that is inversely proportional to `k`.
        * The variance of `ğ“›â‚–(x)` vanishes at a rate that is inversely proportional to `k`.

### Annealed Importance Sampling
* **Annealed Importance Sampling** (AIS) uses Markov chain transitions for an annealing sequence to define an importance sampler.
    * The Markov chain aspect ensures the method performs well in high dimensions.
    * The importance weights ensure that the estimates converge to the correct values.
* The immediate goal of annealed importance sampling is to generate a sample from a target distribution.
    * Let `p(x) = pâ‚€(x) âˆ fâ‚€(x)` be the target distribution.
    * Let `q(x) = pâ‚™(x) âˆ fâ‚™(x)` be the proposal distribution.
    * Let `páµ¢(x)` be an intermediate distributions for each `i` in `{1,...,n - 1}`.
    * Let `Tâ±¼(x, x')` be a Markov chain transition probability from `x` to `x'`.
    * Now, sample an independent point `xâ‚™â‚‹â‚ ~ q(x)`.  Then, sample `xâ‚™â‚‹â‚‚` from `xâ‚™â‚‹â‚` using `Tâ‚™â‚‹â‚`.  Repeat this process until `xâ‚€` is sampled from `xâ‚` using `Tâ‚`.
* The importance weight `w` of a sample is given by `w = Î áµ¢ [fáµ¢â‚‹â‚(xáµ¢â‚‹â‚)/fáµ¢(xáµ¢â‚‹â‚)]`.
        * Naturally, `ğ”¼[x] = (Î£áµ¢ xáµ¢*wáµ¢) / (Î£áµ¢ wáµ¢)`.
* The intermediate distributions are usually taken to be `fáµ¢(x) = fâ‚€(x)áµâ½â±â¾ * fâ‚™(x)Â¹â»áµâ½â±â¾`.
    * Above, `1 = Î²â‚€ > ... > Î²â‚™ = 0`.
* The Markov chain transitions may be constructed using [Metropolis-Hastings sampling](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) or [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) updates.


## References
* [IWAE - Introductory Paper](https://arxiv.org/pdf/1509.00519.pdf)
* [IWAE - Bias and Variance](https://openreview.net/pdf?id=HyZoi-WRb)
* [IWAE - Debiasing Evidence Approximations Video](https://www.youtube.com/watch?v=nRgjvACKNAQ)
* [AIS - Introductory Paper](https://www.cs.toronto.edu/~radford/ftp/ais-rev.pdf)
* [AIS - Blog Tutorial](https://wiseodd.github.io/techblog/2017/12/23/annealed-importance-sampling/)