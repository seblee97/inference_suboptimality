# Inference Suboptimality in Variational Autoencoders
This repository contains code for a project undertaken as part of the Advanced Topics in Machine Learning course (HT 2020) at Oxford. The code here was written by Mikhail Andrenkov, Maxence Draguet, Sebastian Lee, and Diane Magnin.

It is a reproduction of the code for the paper [_Inference Suboptimality in Variational Autoencoders_](https://arxiv.org/pdf/1801.03558.pdf) by Cremer, Li & Duvenaud. 

Our code contains the following features relevant to replicating results from the paper:

* Flexible encoder/decoder architectures
* Various approximate posteriors inlcuding:
    * Factorised Gaussian
    * R-NVP flows
    * R-NVP flows with auxiliary variables
* Relevant binarised image datasets inlcuding:
    * MNIST
    * Fashion-MNIST
    * CIFAR-10
* Local optimisation training loop
* AIS and IWAE log-likelihood estimators

Additionally we implemented a planar flows approximate posterior.

## Quick Links
* [Spreadsheet](https://docs.google.com/spreadsheets/d/1y8K3G4ih2Ta9uB6wM7noJpNtmomSwDDZmKUIKUfGlTk/edit#gid=0)
* [Overleaf Report](https://www.overleaf.com/2537812191smnpkcprxdxs)
* [Experiment List](https://docs.google.com/document/d/1mjVGWMD_I13s5KsolYSpAij37CbMO58AiLj4kKjfKHA/edit?usp=sharing)
* [Research Paper](https://arxiv.org/abs/1801.03558)
* [Conference Poster](https://unioxfordnexus-my.sharepoint.com/:p:/g/personal/hert5869_ox_ac_uk/EeDaJHDlDnhNr4R5VdsCbaoBEgjwqa5hrsuq-I7QbCKh8Q?e=uGHYxV)

## Prerequisites

To run this code you will need the following:

* Python 3.7+

Our code uses PyTorch. We include a requirements file (requirements.txt). We recommend creating a virtual environment (using ```conda``` or ```virtualenv```) for this code base e.g.

```python3 -m venv aml; source aml/bin/activate```

From there, all Python prerequisites should be satisfied by running

```pip3 install -r requirements.txt```

To run experiments with a GPU, it is essential to use Python **3.7.5** (on Windows).  Our code is compatible with CUDA 10.1.

## Datasets

We do not provide the datasets directly in this repository. However we are using modifications of standard datasets (e.g. MNIST, CIFAR10) that can be loaded with the torchvision datasets module. To retrieve the datasets, and make the requisite modifications (the binarisation specified by [Larochelle et al](https://dl.acm.org/doi/abs/10.1145/1390156.1390224)) run:

```python data/get_datasets.py```

## Running Code

Standalone experiments can be run from the experiment folder using the main.py script. Configuration for such an experiment can be set using the base_config.yaml file for general attributes of the experiment as well as specific config files in the additional_configs/ folder (e.g. for setting parameters of a flow module).

Running a specific experiment from the paper can be done by accessing the relevant hard coded configuration files in the Experiment_List folder, which have been made to match the specifications of the paper. For example to reproduce the configuration of a fully-factorised gaussian approximate posterior with an amortised inference network (`ğ“›(VAE[q]) | qFFG` from Table 2. in the paper), run from the experiments folder:

```python main.py -config experiment_list/expA/base_config.yaml -additional_configs experiment_list/expA/additional_configs/```

Alternatively, all results from a given experiment can be run at once in sequence using the bash script in the relevant experiment folder.

## Accessing Experimental Results

Results of an experiment are by default saved in experiments/results/X/ where X is a timestamp for the experiment. Here you will find a copy of the configuration used to run that experiment, a .csv file containing logging of relevant metrics (e.g. train/test loss), and tensorboard events files. To view the tensorboard logs navigate to this folder and run:

```tensorboard --logdir .```

Alternatively run the command from elsewhere and modify the path accordingly. Plots of an experiment run can also be made by running the plot_from_df.py script from the experiments/plotting folder and passing the path to the folder containing the csv file to the -save_path flag.

Weights of the models being trained in a given experiment are also saved by default in experiment/saved_models/Y/X/ where Y is a hash of the configuration file and X is a timestamp for the experiment. Saved models can be loaded (e.g. to run local optimisation) by specifying the saved model path in the base config (Note they are saved weights and not full checkpoints so cannot be used to resume training).

## Code Structure

Below is the structure of the relevant files in our repository. 

```
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚     
â”œâ”€â”€ data
â”‚     
â”œâ”€â”€ experiments
â”‚    â”‚
â”‚    â”‚
â”‚    â”œâ”€â”€ additional_configs
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ aux_flow_config.yaml
â”‚    â”‚   â”œâ”€â”€ esimator_config.yaml
â”‚    â”‚   â”œâ”€â”€ flow_config.yaml
â”‚    â”‚   â”œâ”€â”€ local_optimisation_config.yaml
â”‚    â”‚   â””â”€â”€ planar_config.yaml
â”‚    â”‚
â”‚    â”œâ”€â”€ experiment_list (bash scripts for paper experiments)
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ expA
â”‚    â”‚   â”œâ”€â”€ expB
â”‚    â”‚   â”œâ”€â”€ expB
â”‚    â”‚   â”œâ”€â”€ expC
â”‚    â”‚   â”œâ”€â”€ expD
â”‚    â”‚   â””â”€â”€ expE
â”‚    â”‚
â”‚    â”œâ”€â”€ plotting
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ plot_config.json
â”‚    â”‚   â””â”€â”€ plot_from_df.py
â”‚    â”‚
â”‚    â”œâ”€â”€ results
â”‚    â”‚   â”‚
â”‚    â”‚   â””â”€â”€ **result files (not tracked/commited)**
â”‚    â”‚
â”‚    â”œâ”€â”€ saved_models
â”‚    â”‚   â”‚
â”‚    â”‚   â””â”€â”€ **saved_model files (not tracked/commited)**
â”‚    â”‚
â”‚    â”œâ”€â”€ base_config.yaml
â”‚    â”œâ”€â”€ context.py
â”‚    â””â”€â”€ main.py
â”‚     
â”œâ”€â”€ models
â”‚    â”‚
â”‚    â”‚
â”‚    â”œâ”€â”€ approximate_posteriors
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ base_approximate_posterior.py
â”‚    â”‚   â”œâ”€â”€ base_norm_flow.py
â”‚    â”‚   â”œâ”€â”€ gaussian.py
â”‚    â”‚   â”œâ”€â”€ planar_flow.py
â”‚    â”‚   â”œâ”€â”€ rnvp_aux_flow.py
â”‚    â”‚   â”œâ”€â”€ rnvp_flow.py
â”‚    â”‚   â””â”€â”€ sylv_flow.py
â”‚    â”‚
â”‚    â”œâ”€â”€ likelihood_estimators
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ ais_estimator.yaml
â”‚    â”‚   â”œâ”€â”€ base_estimator.yaml
â”‚    â”‚   â”œâ”€â”€ iwae_estimator.yaml
â”‚    â”‚   â””â”€â”€ max_estimator.yaml
â”‚    â”‚
â”‚    â”œâ”€â”€ local_optimisation_modules
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ base_local_optimisation.py
â”‚    â”‚   â”œâ”€â”€ gaussian_local_optimisation.py
â”‚    â”‚   â”œâ”€â”€ rnvp_aux_flow_local_optimisation.py
â”‚    â”‚   â””â”€â”€ rnvp_flow_local_optimisation.py
â”‚    â”‚
â”‚    â”œâ”€â”€ loss_modules
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ base_loss.py
â”‚    â”‚   â”œâ”€â”€ gaussian_loss.py
â”‚    â”‚   â”œâ”€â”€ planar_loss.py
â”‚    â”‚   â”œâ”€â”€ rnvp_aux_loss.py
â”‚    â”‚   â””â”€â”€ rnvp_loss.py
â”‚    â”‚
â”‚    â”œâ”€â”€ networks
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ __init__.py
â”‚    â”‚   â”‚
â”‚    â”‚   â”œâ”€â”€ base_network.py
â”‚    â”‚   â”œâ”€â”€ convolutional.py
â”‚    â”‚   â”œâ”€â”€ deconvolutional.py
â”‚    â”‚   â”œâ”€â”€ fc_encoder.py
â”‚    â”‚   â””â”€â”€ fc_decoder.py
â”‚    â”‚
â”‚    â”œâ”€â”€ decoder.py
â”‚    â”œâ”€â”€ encoder.py
â”‚    â”œâ”€â”€ vae_runner.py
â”‚    â””â”€â”€ vae.py
â”‚    
â””â”€â”€ utils
     â”‚
     â”‚
     â”œâ”€â”€ __init__.py 
     â”‚     
     â”œâ”€â”€ custom_torch_transforms.py
     â”œâ”€â”€ dataloaders.py
     â”œâ”€â”€ math_operations.py
     â”œâ”€â”€ parameters.py 
     â”œâ”€â”€ torch_operations_test.py
     â””â”€â”€ torch_operations.py             
```
